{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###                 MULTIVARIATE LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will show how employee salaries can be predicted from different employee characteristics (or features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required Packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression \n",
    "import statsmodels\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **This assignment addresses the following:**\n",
    "\n",
    "\n",
    "[Exploratory Data Analysis -EDA](#EDA) \n",
    "<a href='EDA'> </a>\n",
    "\n",
    "[Feature Engineering](#feng)\n",
    "<a href='feng'> </a>\n",
    "\n",
    "[Correlation and Statistical Significance Analysis](#corr)\n",
    "<a href='corr'> </a>\n",
    "\n",
    "[Feature Selection](#select)\n",
    "<a href='select'> </a>\n",
    "\n",
    "[Model Training](#train)\n",
    "<a href='train'> </a>\n",
    "\n",
    "[Predictions](#predict)\n",
    "<a href='predict'> </a>\n",
    "\n",
    "\n",
    "[Model Evaluation](#eval)\n",
    "<a href='eval'> </a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='EDA'></a> <div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Exploratory Data Analysis (EDA)\n",
    "- Data Ingestion\n",
    "- Data Preprocessing\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "sal = pd.read_csv('data/salary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine DataSet\n",
    "sal.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Our target variable (salary) has one missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to fix missing value\n",
    "sal['salary'].fillna(sal['salary'].mean(),inplace = True)\n",
    "#sal = sal.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Missing value has been filled w/ mean. Duplicates would be dropped if dataset had any \n",
    "* Filled w/ mean instead of dropping because salary is the target variable and don't want to miss any insights it might bring even when it's just one value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert floats to ints\n",
    "sal = sal.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the missing value has been dealt with I have converted all the columns to integers (info method above shows us that 2 columns (salary&market) are floats.) I chose to convert because down the line when we calculate metrics and compare our predictions, it's better to work with whole numbers versus floats that need to be rounded off.\n",
    "\n",
    "__Note that astype() will raise an error when it encounters NaN values so be sure to deal w/ them first before proceeding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The above table tells us the number of observations (514). We expect salary to have 514 because the missing value is now represented by the mean.\n",
    "* The descriptive statistics table also tells us about the means, standard deviations, min and max values as well as the percentiles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='feng'></a> <div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Feature Engineering\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "field = ['Engineering', 'Finance', 'Human Resources', 'Marketing']\n",
    "trace = go.Pie(labels = field, values = sal.Field)\n",
    "data = [trace]\n",
    "layout = go.Layout(\n",
    "   {\"title\":\"Career Fields\"})\n",
    "fig = go.Figure(data,layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this graph is to have an understanding of which career field are the most dominant within the dataset. The Marketing profession accounts for 33.3%, Eng & HR are tied at second place leaving Finance in last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(sal['Field'])\n",
    "\n",
    "dum = dummies.rename(columns={1:'engineering',2:'finance',3:'HR',4:'marketing'})\n",
    "\n",
    "sal2 = pd.concat([sal, dum] ,axis=1, ignore_index=False)\n",
    "sal2.drop(\"Field\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='corr'></a> <div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Correlation Analysis and Statistical Significance \n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Pearson correlation coeffificent and plot the corresponding correlation matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#omitting categorical variables\n",
    "sal_corr = sal.drop([\"Field\", 'position'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"For a dichotomous categorical variable and a continuous variable you can calculate a Pearson correlation if the categorical variable has a 0/1-coding for the categories. ... But when you have more than two categories for the categorical variable the Pearson correlation is not appropriate anymore.\"\n",
    "ref: https://www.researchgate.net/post/Can_I_use_Pearsons_correlation_coefficient_to_know_the_relation_between_perception_and_gender_age_income#:~:text=For%20a%20dichotomous%20categorical%20variable,1%2Dcoding%20for%20the%20categories.&text=But%20when%20you%20have%20more,correlation%20is%20not%20appropriate%20anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_corr.corr()\n",
    "sal_corr.style.background_gradient(cmap = 'Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Everything has a correlation but it is the strength of the correlation that we are interested in.__\n",
    "* Based on the above table these features are good predictors for salary; yearsworked, yearsrank, position and Field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'p-value & correlation coef of yearsworked and salary: {stats.pearsonr(sal.yearsworked, sal.salary)}')\n",
    "print(f'p-value & correlation coef of yearsrank and salary: {stats.pearsonr(sal.yearsrank, sal.salary)}')\n",
    "print(f'p-value & correlation coef of postion and salary: {stats.pearsonr(sal.position, sal.salary)}')\n",
    "print(f'p-value & correlation coef of Field and salary: {stats.pearsonr(sal.Field, sal.salary)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='select'></a> <div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Feature Selection\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank={1:'Junior', 2:'Manager', 3:'Executive'}\n",
    "sal2['position']=sal2.position.map(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal2 = sal.drop(['exprior', 'market', 'degree', 'otherqual', 'male', 'yearsabs'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Check for Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote> \"Multicollinearity exists whenever an independent variable is highly correlated with one or more of the other independent variables in a multiple regression equation.\" - (Taken from the book 'Understanding Regression Analysis')\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step 1: Check correlation between selected features. (i.e identify variables affected by multicollinearity)\n",
    "- Step 2: Calculate the VIF factors\n",
    "(The Variance Inflation Factor (VIF) is a measure of colinearity among predictor variables within a multiple regression.)\n",
    "- Step 3: Inspect the factors for each predictor variable, if the VIF is between 5-10, multicolinearity is likely present and you should consider dropping the variable.\n",
    "\n",
    "\n",
    "#### [Read More Here](https://newprairiepress.org/cgi/viewcontent.cgi?referer=https://www.google.com/&httpsredir=1&article=1034&context=agstatconference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step1\n",
    "m_coll = sal2[['yearsworked', 'yearsrank', 'position', 'Field']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step2\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "#calculate VIF and save in dataframe\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(m_coll.values, i) for i in range(m_coll.shape[1])]\n",
    "vif[\"features\"] = m_coll.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Interpretaion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VIF ranges from 1 upwards. \n",
    "- generally a VIF above 10 indicates high correlation \n",
    "- if you have high VIFs for dummy variables representing nominal variables with three or more categories, those are usually not a problem.\n",
    "\n",
    "Judging by the correlation matrix below, yearsworked is highly correlated with position (0.75) & yearsrank (0.81). This is the variable that'll be removed before moving on to model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_corr = sal2.corr()\n",
    "features_corr.style.background_gradient(cmap = 'Oranges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sal2 = sal2.drop(['yearsworked'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='train'></a> <div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Model Training\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dataset is split into a train & test set. \n",
    "70% of the data will go into the training set and the remaining 30% will be used for testing.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "x = sal2.drop('salary', axis=1)\n",
    "y = sal2[\"salary\"]\n",
    "\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size=0.3, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols('salary ~ yearsrank + position + Field', sal2).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-squared value reveals the quality of the regression model. It describes the relationships between dependent and independent variables in a model. The R-squared value for this model is 0.66. This means the accuracy of the model is approx 66%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='predict'></a> <div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Predictions & Model Testing\n",
    "\n",
    "Predicting salary using the test set.\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_salary = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the __X_test__ data to pass in features the model has never seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataframe for predicted values\n",
    "df = pd.DataFrame(predict_salary)\n",
    "df.rename(columns={0:'predicted_salary'},inplace=True)\n",
    "df = df.astype(int)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = pd.concat([Y_test,df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model has made reasonable predictions. Lets look at the figures in index 214 for example. Salary had a value of 59 472 and the model predicted 59 570! That's Incredible!\n",
    "- The model has an accuracy rate of 66% and some estimations may be off. The values in index 206 are one such example.\n",
    "- A conclusion we can make from this is that although the model needs some fine-tuning, the chosen predictors can help in predicting a person's salary.\n",
    "- The model can be generalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse Distribution of Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot((Y_test - predict_salary))\n",
    "plt.title(\"Distribution of Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The figure is normally distributed\n",
    "- This is a good sign because it means this model is a correct choice for the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eval'></a> <div class=\"alert alert-block alert-info\">\n",
    "\n",
    "#### Model Evaluations\n",
    "Regression Evaluation Metrics\n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 common evaluation metrics for regression problems:\n",
    "\n",
    "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n",
    "\n",
    "**Mean Squared Error** (MSE) is the mean of the squared errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n",
    "\n",
    "$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n",
    "\n",
    "All of these are **loss functions**. We want to minimize them to create the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE:', int(metrics.mean_squared_error(Y_test, predict_salary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('testRMSE:', int(np.sqrt(metrics.mean_squared_error(Y_test, predict_salary))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('trainsetRMSE:', int(np.sqrt(metrics.mean_squared_error(Y_train, model.predict(X_train)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RMSE indicates the absolute fit of the model to the data (how close the actual data points are to the model's predicted ones.)\n",
    "- Test RMSE is bigger than the Trainset RMSE\n",
    "- RMSE is a good measure of how accurately the model predicts the target variable. It is the most important criteria for fit if the main purpose of the model is prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Absolute Error(MAE) is one of the many metrics for summarizing and assessing the quality of a machine learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "print('MAE:', int(metrics.mean_absolute_error(Y_test, predict_salary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(Y_test, predict_salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The mean absolute percentage error (MAPE) is a statistical measure of how accurate a forecast system is. It measures this accuracy as a percentage.\n",
    "- (MAPE) works best if there are no extremes to the data\n",
    "- Since MAPE is a measure of error, high numbers are bad and low numbers are good (meaning that our 12.2% is something to be happy about)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(Y_test, predict_salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Error Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from yellowbrick.regressor import PredictionError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = Lasso()\n",
    "visualizer = PredictionError(mod)\n",
    "\n",
    "#Fit training data to visualizer\n",
    "visualizer.fit(X_train, Y_train)\n",
    "\n",
    "#Evaluate model on the test data\n",
    "visualizer.score(X_test, Y_test)\n",
    "\n",
    "visualizer.show()                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A __Prediction Error Plot__ shows the actual vales from the dataset against the predicted values generated by the model.\n",
    "- This allows us to see how much variance is in the model.\n",
    "- The Line of Best Fit is used to express a relationship in a scatter plot of different data points.\n",
    "- It is an output of regression analysis and can be used as a prediction tool for indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the standardised residuals resid() & standardised predicted values fittedvalues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals  = model.resid\n",
    "\n",
    "fittedv   = model.fittedvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the residuals versus the predicted values using seabornâ€™s residplot with fitted values as the x parameter, and the dependent variable as y, specify lowess=True. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.residplot(fittedv,residuals, lowess=True, color='maroon')\n",
    "plt.title('Residuals vs Predicted Values')\n",
    "plt.ylabel('Fitted Values')\n",
    "plt.xlabel('Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
